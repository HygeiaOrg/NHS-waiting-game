{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0867b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting snscrape\n",
      "  Downloading snscrape-0.4.3.20220106-py3-none-any.whl (59 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/lib/python3/dist-packages (from snscrape) (4.10.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: lxml in /usr/lib/python3/dist-packages (from snscrape) (4.8.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/lib/python3/dist-packages (from snscrape) (2.25.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/slisowski/.local/lib/python3.10/site-packages (from requests[socks]->snscrape) (1.7.1)\n",
      "Installing collected packages: filelock, snscrape\n",
      "Successfully installed filelock-3.8.0 snscrape-0.4.3.20220106\n"
     ]
    }
   ],
   "source": [
    "!pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f7f91543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/slisowski/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import snscrape.modules.twitter as snt\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import os\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b5011c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_lexicon=pd.read_csv('/path/to/query_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a2e4f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_lexicon=query_lexicon['terms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8e3ade5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           commit suicide\n",
       "1        jump off a bridge\n",
       "2       i want to overdose\n",
       "3             i’m a burden\n",
       "4        i’m such a burden\n",
       "              ...         \n",
       "59                 anxiety\n",
       "60         eating disorder\n",
       "61               depressed\n",
       "62    don't want this life\n",
       "63                     NaN\n",
       "Name: terms, Length: 64, dtype: object"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "34723be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: snscrape twitter-user [-h] [--user-id] username\r\n",
      "snscrape twitter-user: error: the following arguments are required: username\r\n"
     ]
    }
   ],
   "source": [
    "!snscrape twitter-user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "74dc08ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[\n",
    "    \"query\",\n",
    "    \"tweet_id\",\n",
    "    \"date_time\",\n",
    "    \"lang\",\n",
    "    \"username\",\n",
    "    \"user_desc\",\n",
    "    \"tweet_content\",\n",
    "    \"location\",\n",
    "    \n",
    "    \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f68c4fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_geo(query,limit,start_date,end_date,geocode):\n",
    "   \"\"\"\n",
    "   Return the \"labels\" as a dataframe while you take the inputs of time,query and the radius within london\n",
    "   \"\"\"\n",
    "   #empty list that will store everything\n",
    "   df=[]\n",
    "   for i,tweet in enumerate(snt.TwitterSearchScraper(f'{query} since:{start_date} until:{end_date} geocode:\"{geocode}\"').get_items()):\n",
    "      \n",
    "      tdf=[\n",
    "         query,\n",
    "         tweet.id,\n",
    "         tweet.date,\n",
    "         tweet.lang,\n",
    "         tweet.user.username,\n",
    "         tweet.user.description,\n",
    "         tweet.content,\n",
    "         tweet.user.location\n",
    "         ]\n",
    "      \n",
    "      df.append(tdf)\n",
    "      tdf=[]\n",
    "      if i==limit-1:\n",
    "        break\n",
    "   pdf=pd.DataFrame(df,columns=labels)\n",
    "   pdf=pdf[pdf[\"lang\"]=='en']\n",
    "   pdf['is_london']=pdf['location'].apply(lambda x : re.match('london', x, re.IGNORECASE))\n",
    "   pdf=pdf[~pdf['is_london'].isna()]\n",
    "   pdf=pdf[[\"query\",\"tweet_id\",\"date_time\",\"lang\",\"username\",\"user_desc\",\"tweet_content\",\"location\"]]\n",
    "   return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5785309f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████████████████████████▎                 | 37/63 [12:47<06:34, 15.16s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "dfs=[]\n",
    "loc = '51.505234,-0.115362, 50km'\n",
    "for query in tqdm(query_lexicon[:-1]):\n",
    "    \n",
    "    query='\"{}\"'.format(query)\n",
    "    df=tweet_geo(query, 1000, '2015-11-11', '2022-11-11', loc)\n",
    "    file=''\n",
    "    query=query.replace('\"','')\n",
    "    token_query=word_tokenize(query)\n",
    "    if len(token_query)>1:\n",
    "        file='_'.join(token_query)\n",
    "        file=file+'.csv'\n",
    "    else:\n",
    "        file=query+'.csv'\n",
    "   \n",
    "    file='/path/to/'+file\n",
    "    df.to_csv(file)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164fbdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bca385",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=all_data.drop_duplicates(subset=['tweet_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=all_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7073476",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv('/path/to/file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0829ba1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e1771f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/slisowski/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/slisowski/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import preprocessor as p\n",
    "from bs4 import BeautifulSoup\n",
    "import demoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "lem = WordNetLemmatizer()\n",
    "ste=PorterStemmer()\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def clean_tweet(text):\n",
    "    \n",
    "    #remove emoji\n",
    "    text=demoji.replace(text,' ')\n",
    "    # use tweeter preprocessor to clean news\n",
    "    text=p.clean(text)\n",
    "    #remove html tags\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    text=contractions.fix(text)\n",
    "    \n",
    "    \n",
    "    #remove accented characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    text=text.lower()\n",
    "   \n",
    "    #remove numbers\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    #tokenize text\n",
    "    \n",
    "    list_of_words=word_tokenize(text)\n",
    "    \n",
    "    text = ' '.join([lem.lemmatize(word) for word in list_of_words])\n",
    "    text=text.lower()\n",
    "    \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2689987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slisowski/.local/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator CountVectorizer from version 1.1.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/slisowski/.local/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.1.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/slisowski/.local/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 1.1.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "pipeline_tweet=load('/home/slisowski/Portfolio/omdena_self_harm_pred/tweets_to_model/tweets_pipeline_logreg.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b8a3c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(tweet):\n",
    "    cleaned_tweet=clean_tweet(tweet)\n",
    "    predictions=pipeline_tweet.predict([cleaned_tweet])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb91f56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_class(\"Tomorrow is World Suicide Prevention  Day #WSPD a day for all of us to commit to doing all we can to prevent these terrible losses  #SuicidePrevention\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df7163e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_class('People spreading the idea that you must censor \"lesbian\" in social media will kill me one day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77160a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_class(\"My wife’s uncle committed suicide this morning after struggling with mental health issues. If you are struggling, please reach out and talk to someone. It has helped me in the past and it can help you as well.Don’t suffer in silence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b7c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
